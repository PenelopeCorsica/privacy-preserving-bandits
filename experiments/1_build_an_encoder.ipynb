{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16157,
     "status": "ok",
     "timestamp": 1581422707405,
     "user": {
      "displayName": "M. M.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDWyjiBHdeijAV12uYubfUgHbdnQEzBok62Adg=s64",
      "userId": "11054559934326118576"
     },
     "user_tz": 0
    },
    "id": "Rxbtoe6v4i-b",
    "outputId": "39a3f30a-94e6-458c-de68-c2ef530c1032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting iteround==1.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/35/1d/ad1084701aa0e259a4e7f343ec32c94b2c799c377dadb2326133065275c9/iteround-1.0.2-py3-none-any.whl\n",
      "Installing collected packages: iteround\n",
      "Successfully installed iteround-1.0.2\n",
      "Collecting pairing==0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/04/8b/fcc466d4a0ad67eb4bf2ca58b5d540d5c6d0ec0706c03a9596bcade89334/pairing-0.1.3.tar.gz\n",
      "Building wheels for collected packages: pairing\n",
      "  Building wheel for pairing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pairing: filename=pairing-0.1.3-cp36-none-any.whl size=2135 sha256=05e8f50ab4ddada0e3962238d271f0bc38df68f2572ca453913b00456a1bed10\n",
      "  Stored in directory: /root/.cache/pip/wheels/a2/d8/9b/b97f0f0dac35d102b3671a0e0418f4d47493c0a97118172fc0\n",
      "Successfully built pairing\n",
      "Installing collected packages: pairing\n",
      "Successfully installed pairing-0.1.3\n",
      "Collecting scikit-multilearn==0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
      "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
      "Successfully installed scikit-multilearn-0.2.0\n",
      "Collecting arff==0.9\n",
      "  Downloading https://files.pythonhosted.org/packages/50/de/62d4446c5a6e459052c2f2d9490c370ddb6abc0766547b4cef585913598d/arff-0.9.tar.gz\n",
      "Building wheels for collected packages: arff\n",
      "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for arff: filename=arff-0.9-cp36-none-any.whl size=4970 sha256=1a7eeb974fe995f8300ef1bcd2037d051c7dacc5ca28e8335b9646a81caf5345\n",
      "  Stored in directory: /root/.cache/pip/wheels/04/d0/70/2c73afedd3ac25c6085b528742c69b9587cbdfa67e5194583b\n",
      "Successfully built arff\n",
      "Installing collected packages: arff\n",
      "Successfully installed arff-0.9\n",
      "Collecting category_encoders==2.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/52/c54191ad3782de633ea3d6ee3bb2837bda0cf3bc97644bb6375cf14150a0/category_encoders-2.1.0-py2.py3-none-any.whl (100kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (1.17.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (0.5.1)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (0.25.3)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders==2.1.0) (0.10.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders==2.1.0) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders==2.1.0) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders==2.1.0) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders==2.1.0) (2018.9)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install iteround==1.0.2\n",
    "!pip install pairing==0.1.3\n",
    "!pip install scikit-multilearn==0.2.0\n",
    "!pip install arff==0.9\n",
    "!pip install category_encoders==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRCXMRe-ijVa"
   },
   "outputs": [],
   "source": [
    "## Basics\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import iteround\n",
    "import itertools\n",
    "import numpy as np\n",
    "from scipy import special\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "class Hasher():\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    os.environ['PYTHONHASHSEED']=str(0)\n",
    "    \"\"\"\n",
    "    comb_algo : \"rec\", \"prod\"\n",
    "    \n",
    "    cluster_algo: \"kmean\", \"mbkmean\"\n",
    "    \"\"\"\n",
    "    def __init__(self, comb_algo = \"rec\", cluster_algo= \"kmean\"):\n",
    "        self.comb_algo = comb_algo\n",
    "        self.cluster_algo = cluster_algo\n",
    "\n",
    "    def combinations_prod(self, n, tot):\n",
    "        #### src: https://codereview.stackexchange.com/questions/190122/permutations-with-a-sum-constraint\n",
    "        def combinations_prod_inner(n, tot):\n",
    "            items = list(range(0,tot+1,1))\n",
    "            combinations = pd.DataFrame(list(filter(lambda x: np.sum(x)==tot, \n",
    "                                                  list(itertools.product(items, repeat=n)))))\n",
    "            return(combinations.as_matrix())\n",
    "\n",
    "        res = combinations_prod_inner(n, tot)\n",
    "        res = np.array(res)\n",
    "        res = res/res.sum(axis=1)[:, np.newaxis]\n",
    "        return res\n",
    "\n",
    "\n",
    "    def combinations_recursive(self, n, tot):\n",
    "        #### src: https://codereview.stackexchange.com/questions/190122/permutations-with-a-sum-constraint\n",
    "        def combinations_recursive_inner(n, buf, gaps, tsum, accum, tot):\n",
    "            if gaps == 0:\n",
    "                accum.append(list(buf))\n",
    "            else:\n",
    "                for x in range(0, tot+1):\n",
    "                    if tsum + x + (gaps - 1) * tot < tot:\n",
    "                        continue\n",
    "                    if tsum + x > tot:\n",
    "                        break\n",
    "                    combinations_recursive_inner(n, buf + [x], gaps - 1, tsum + x, accum, tot)\n",
    "        \n",
    "        res = []\n",
    "        combinations_recursive_inner(n, [], n, 0, res, tot)\n",
    "        res = np.array(res)\n",
    "        res = res/res.sum(axis=1)[:, np.newaxis]\n",
    "        return res\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_hasher(self, context_size, bin_size, dec_digits=1, saving=True):\n",
    "        ### src:https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)\n",
    "        num_tot_hsits = special.comb(((10**dec_digits)+context_size-1), context_size-1)\n",
    "        print(\"Total number of possible contexts(histograms):\", num_tot_hsits)\n",
    "        if self.comb_algo == \"rec\":\n",
    "            all_hists = self.combinations_recursive(context_size, 10**dec_digits)\n",
    "        elif self.comb_algo == \"prod\":\n",
    "            all_hists = self.combinations_prod(context_size, 10**dec_digits)\n",
    "        \n",
    "        #### To clustering histograms\n",
    "        print(\"Clustering...\")\n",
    "        if self.cluster_algo == \"kmean\":\n",
    "            kmeans = KMeans(n_clusters=2**bin_size,\n",
    "                            n_jobs = -1,\n",
    "                            random_state=0).fit(all_hists)\n",
    "        elif self.cluster_algo == \"mbkmean\":\n",
    "            kmeans = MiniBatchKMeans(n_clusters=2**bin_size, \n",
    "                                     batch_size = bin_size*bin_size,\n",
    "                                     init_size=2**bin_size,\n",
    "                                     n_init=bin_size,\n",
    "                                     random_state=0).fit(all_hists)\n",
    "        \n",
    "        #### To order clustering labels from highest to lowest\n",
    "        idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "        re_indexer = np.zeros_like(idx)\n",
    "        re_indexer[idx] = np.arange(2**bin_size)\n",
    "        \n",
    "        if saving:\n",
    "            print(\"Saving...\")\n",
    "            save_dir = \"encoders_repo\"\n",
    "            f_name =  \"hasher_\"+str(context_size)+\"_\"+str(dec_digits)+\"_\"+str(bin_size)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            with open(save_dir+\"/kmeans_\"+f_name+\".pkl\", 'wb') as fid:\n",
    "                pickle.dump(kmeans, fid)\n",
    "            np.save(save_dir+\"/re_indexer_\"+f_name+\".npy\", re_indexer)\n",
    "        \n",
    "        print(\"Completed!\")\n",
    "        return kmeans, re_indexer\n",
    "    \n",
    "    def get_hasher(self, context_size, bin_size, dec_digits=1):\n",
    "        save_dir = \"encoders_repo\"\n",
    "        f_name =  \"hasher_\"+str(context_size)+\"_\"+str(dec_digits)+\"_\"+str(bin_size)\n",
    "        with open(save_dir+\"/kmeans_\"+f_name+\".pkl\", 'rb') as fid:\n",
    "            kmeans = pickle.load(fid)\n",
    "        re_indexer = np.load(save_dir+\"/re_indexer_\"+f_name+\".npy\")\n",
    "        \n",
    "        return kmeans, re_indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WBgE8zJ-Lck"
   },
   "source": [
    "## Building an Encoder\n",
    "To build your own encoder, you need to set the following three values:\n",
    "1. `context_size` which is called `d` in the paper.\n",
    "2. `bin_size` which is `log_2 (k)` in the paper. (in the paper we use three different values for `k`: 2^5,2^7, and 2^10)\n",
    "3. `dec_digits` whic is called `q`.\n",
    "\n",
    "You also can use either `kmean` or `mbkmean`. The former is much more accurate, however the latter is much more faster.\n",
    "> more info: https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15966,
     "status": "ok",
     "timestamp": 1581422737617,
     "user": {
      "displayName": "M. M.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDWyjiBHdeijAV12uYubfUgHbdnQEzBok62Adg=s64",
      "userId": "11054559934326118576"
     },
     "user_tz": 0
    },
    "id": "4hEJ25tMijVc",
    "outputId": "d9e86999-2e55-43fa-a0a2-ff5d657f0957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possible contexts(histograms): 92378.0\n",
      "Clustering...\n",
      "Saving...\n",
      "Completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "        n_clusters=32, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "        random_state=0, tol=0.0001, verbose=0),\n",
       " array([21,  6, 14,  2, 12,  4,  5,  9,  8, 25, 22, 30, 31, 20,  7, 18,  3,\n",
       "        17, 15, 13, 24, 28,  0, 29,  1, 10, 19, 11, 16, 27, 26, 23]))"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher = Hasher(comb_algo = \"rec\", cluster_algo= \"kmean\")\n",
    "hasher.build_hasher(context_size=10, ## This is `d` in the paper\n",
    "                    bin_size=5, ## This is `log_2(k)` in the paper\n",
    "                    dec_digits=1, ## This is `q` in the paper\n",
    "                    saving=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1581422740758,
     "user": {
      "displayName": "M. M.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBDWyjiBHdeijAV12uYubfUgHbdnQEzBok62Adg=s64",
      "userId": "11054559934326118576"
     },
     "user_tz": 0
    },
    "id": "RnFNPTyB_jbo",
    "outputId": "92b7abb7-985b-4e37-f4d6-017f3737a605"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "        n_clusters=1024, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "        random_state=0, tol=0.0001, verbose=0),\n",
       " array([248,  67, 413, ..., 369, 726, 528]))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasher.get_hasher(context_size=10, bin_size=10,dec_digits=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuY9-65N9jAG"
   },
   "source": [
    "### What is the Next Step?\n",
    "\n",
    "Build as much as **Encoder** you need for your desired `context_size`, `bin_size`, and `dec_digits`. Then you can run one of the following notebooks:\n",
    "* 2_synthetic_exp.ipynb\n",
    "* 2_mlc_exp.ipynb\n",
    "* 2_criteo_exp.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_build_an_encoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
